{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Way to Use KafkaProducer in kafka-python   \n",
    "\n",
    "#pip install kafka \n",
    "#pip install kafka-python    \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='10.222.68.223:9092')\n",
    "producer.send('Test10', b'Hello, Kafka!')\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "kafka_props = {\n",
    "    'bootstrap_servers': '10.222.68.223:9092',\n",
    "    'key_serializer': str.encode,\n",
    "    'value_serializer': str.encode\n",
    "}\n",
    "\n",
    "producer = KafkaProducer(**kafka_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Basic Example of KafkaConsumer\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Create a consumer instance\n",
    "consumer = KafkaConsumer(\n",
    "    'Test10',\n",
    "    bootstrap_servers='10.222.68.223:9092',\n",
    "    auto_offset_reset='earliest', # or 'latest'\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my-group',\n",
    "    value_deserializer=lambda x: x.decode('utf-8')\n",
    ")\n",
    "\n",
    "# Consume messages\n",
    "for message in consumer:\n",
    "    print(f\"Topic: {message.topic}, Partition: {message.partition}, Offset: {message.offset}\")\n",
    "    print(f\"Key: {message.key}, Value: {message.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_props = {\n",
    "    'bootstrap_servers': '10.222.68.223:9092',\n",
    "    'key_serializer': str.encode,\n",
    "    'value_serializer': str.encode\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(**kafka_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    producer.send(\"CustomerCountry\", key=\"Precision Products\", value=\"France\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install confluent-kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': '10.222.68.223:9092',\n",
    "    'client.id': 'my_producer'\n",
    "}\n",
    "producer = Producer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.produce(topic='my_topic', key='my_key', value='my_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced message to CustomerCountry [0] @ offset 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "class DemoProducerCallback:\n",
    "    def __call__(self, err, msg):\n",
    "        if err is not None:\n",
    "            print(f\"Error: {err}\")\n",
    "        else:\n",
    "            print(f\"Produced message to {msg.topic()} [{msg.partition()}] @ offset {msg.offset()}\")\n",
    "\n",
    "producer = Producer({'bootstrap.servers': '10.222.68.223:9092'})\n",
    "\n",
    "record = {'topic': 'CustomerCountry', 'key': 'Biomedical Materials', 'value': 'BRAZIL'}\n",
    "producer.produce(record['topic'], key=record['key'], value=record['value'], callback=DemoProducerCallback())\n",
    "\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fakerNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading Faker-35.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: confluent-kafka[avro] in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: avro<2,>=1.11.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from confluent-kafka[avro]) (1.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from confluent-kafka[avro]) (2.32.3)\n",
      "Requirement already satisfied: fastavro>=1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from confluent-kafka[avro]) (1.9.5)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from faker) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->confluent-kafka[avro]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->confluent-kafka[avro]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->confluent-kafka[avro]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->confluent-kafka[avro]) (2024.7.4)\n",
      "Downloading Faker-35.2.2-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.6/1.9 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-35.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install confluent-kafka[avro] faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'record',\n",
       " 'name': 'Customer',\n",
       " 'namespace': 'com.example',\n",
       " 'fields': [{'name': 'id', 'type': 'string'},\n",
       "  {'name': 'name', 'type': 'string'},\n",
       "  {'name': 'email', 'type': 'string'},\n",
       "  {'name': 'age', 'type': 'int'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Define the Avro Schema\n",
    "\n",
    "{  \n",
    " \"type\": \"record\",\n",
    "  \"name\": \"Customer\",\n",
    "  \"namespace\": \"com.example\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"id\", \"type\": \"string\"},    \n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"email\", \"type\": \"string\"},\n",
    "    {\"name\": \"age\", \"type\": \"int\"}\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer_generator.py\n",
    "customer_generator_code = \"\"\"\n",
    "from faker import Faker\n",
    "\n",
    "class CustomerGenerator:\n",
    "    def __init__(self):\n",
    "        self.fake = Faker()\n",
    "\n",
    "    def generate_customer(self):\n",
    "        return {\n",
    "            'name': self.fake.name(),\n",
    "            'email': self.fake.email(),\n",
    "            'address': self.fake.address(),\n",
    "            'phone_number': self.fake.phone_number(),\n",
    "            'birthdate': self.fake.date_of_birth(minimum_age=18, maximum_age=90).isoformat()\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "with open('customer_generator.py', 'w') as f:\n",
    "    f.write(customer_generator_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files 'customer_generator.py' and 'kafka_producer.py' have been created.\n"
     ]
    }
   ],
   "source": [
    "# Create kafka_producer.py\n",
    "kafka_producer_code = \"\"\"\n",
    "from confluent_kafka import SerializingProducer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from customer_generator import CustomerGenerator\n",
    "\n",
    "# Define the Avro schema for customer data\n",
    "customer_schema_str = '''\n",
    "{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Customer\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"},\n",
    "        {\"name\": \"address\", \"type\": \"string\"},\n",
    "        {\"name\": \"phone_number\", \"type\": \"string\"},\n",
    "        {\"name\": \"birthdate\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Configuration for Schema Registry\n",
    "schema_registry_conf = {\n",
    "    'url': 'http://10.222.68.223:8081'  # Replace with your Schema Registry URL\n",
    "}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# Avro serializer for customer data\n",
    "avro_serializer = AvroSerializer(\n",
    "    schema_registry_client,\n",
    "    customer_schema_str,\n",
    "    to_dict=lambda obj, ctx: obj  # Assuming the customer data is already in dictionary format\n",
    ")\n",
    "\n",
    "# Configuration for Kafka producer\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Replace with your Kafka broker URL\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer': avro_serializer\n",
    "}\n",
    "\n",
    "producer = SerializingProducer(producer_conf)\n",
    "\n",
    "# Function to produce customer data to Kafka\n",
    "def produce_customer_data():\n",
    "    customer_generator = CustomerGenerator()\n",
    "    customer_data = customer_generator.generate_customer()\n",
    "    producer.produce(topic='customer_topic', key=customer_data['email'], value=customer_data)\n",
    "    producer.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    produce_customer_data()\n",
    "\"\"\"\n",
    "\n",
    "with open('kafka_producer.py', 'w') as f:\n",
    "    f.write(kafka_producer_code)\n",
    "\n",
    "print(\"Files 'customer_generator.py' and 'kafka_producer.py' have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastavroNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading fastavro-1.9.5-cp38-cp38-win_amd64.whl (550 kB)\n",
      "Installing collected packages: fastavro\n",
      "Successfully installed fastavro-1.9.5\n"
     ]
    }
   ],
   "source": [
    "pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "from confluent_kafka import SerializingProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Schema Registry configuration\n",
    "schema_registry_conf = {'url': 'http://10.222.68.223:8081'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl (99 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 requests-2.32.3 urllib3-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your Avro schema (as a string or load from file)\n",
    "avro_schema_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Customer\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"id\", \"type\": \"string\"},\n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"email\", \"type\": \"string\"}\n",
    "     ]\n",
    "} \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Avro serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, avro_schema_str)\n",
    "\n",
    "# Kafka producer configuration\n",
    "producer_config = {\n",
    "    'bootstrap.servers': '10.222.68.223:9092',\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer': avro_serializer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "producer = SerializingProducer(producer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import uuid\n",
    "\n",
    "# Define the value to send (must match the Avro schema)\n",
    "customer_data = {\n",
    "    \"id\": str(uuid.uuid4()),\n",
    "    \"name\": \"Alice Johnson\",\n",
    "    \"email\": \"alice.johnson@example.com\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery successful: 099eedf5-ff04-4af3-9977-524eb350346d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Send the message\n",
    "producer.produce(\n",
    "    topic=\"customers\",\n",
    "    key=customer_data[\"id\"],\n",
    "    value=customer_data,\n",
    "    on_delivery=lambda err, msg: print(\n",
    "         f\"Delivery {'failed: ' + str(err) if err else 'successful: ' + msg.key().decode('utf-8')}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wait for all messages to be delivered\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: avro in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.11.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_servers = '10.222.68.223:9092'\n",
    "schema_registry_url = 'http://10.222.68.223:8081/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"MyRecord\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"field1\", \"type\": \"string\"}\n",
    "            # Add other fields as needed\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema_registry_url = 'http://10.222.68.223:8081'\n",
    "#schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
    "\n",
    "schema_registry_conf = {'url': 'http://10.222.68.223:8081'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define schema and data\n",
    "\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "#value_schema = avro.loads(value_schema_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avro_serializer = MessageSerializer(schema_registry_client)\n",
    "\n",
    "\n",
    "# Create Avro serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, value_schema_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}\n",
    "#serialized_data = avro_serializer.encode_record_with_schema(value_schema_str, user_data)\n",
    "\n",
    "\n",
    "# Serialize the data\n",
    "serialized_data = avro_serializer(user_data, SerializationContext(\"MyTopic3\", MessageField.VALUE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
