{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Way to Use KafkaProducer in kafka-python   \n",
    "\n",
    "#pip install kafka \n",
    "#pip install kafka-python    \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='10.222.68.223:9092')\n",
    "producer.send('Test10', b'Hello, Kafka!')\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "kafka_props = {\n",
    "    'bootstrap_servers': '10.222.68.223:9092',\n",
    "    'key_serializer': str.encode,\n",
    "    'value_serializer': str.encode\n",
    "}\n",
    "\n",
    "producer = KafkaProducer(**kafka_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Basic Example of KafkaConsumer\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Create a consumer instance\n",
    "consumer = KafkaConsumer(\n",
    "'test10',\n",
    "    bootstrap_servers='10.222.68.223:9092',\n",
    "    auto_offset_reset='earliest', # or 'latest'\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my-group',\n",
    "    value_deserializer=lambda x: x.decode('utf-8')\n",
    ")\n",
    "\n",
    "# Consume messages\n",
    "for message in consumer:\n",
    "    print(f\"Topic: {message.topic}, Partition: {message.partition}, Offset: {message.offset}\")\n",
    "    print(f\"Key: {message.key}, Value: {message.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_props = {\n",
    "    'bootstrap_servers': '10.222.68.223:9092',\n",
    "    'key_serializer': str.encode,\n",
    "    'value_serializer': str.encode\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(**kafka_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    producer.send(\"CustomerCountry\", key=\"Precision Products\", value=\"France\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install confluent-kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': '10.222.68.223:9092',\n",
    "    'client.id': 'my_producer'\n",
    "}\n",
    "producer = Producer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.produce(topic='my_topic', key='my_key', value='my_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced message to CustomerCountry [0] @ offset 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "class DemoProducerCallback:\n",
    "    def __call__(self, err, msg):\n",
    "        if err is not None:\n",
    "            print(f\"Error: {err}\")\n",
    "        else:\n",
    "            print(f\"Produced message to {msg.topic()} [{msg.partition()}] @ offset {msg.offset()}\")\n",
    "\n",
    "producer = Producer({'bootstrap.servers': '10.222.68.223:9092'})\n",
    "\n",
    "record = {'topic': 'CustomerCountry', 'key': 'Biomedical Materials', 'value': 'BRAZIL'}\n",
    "producer.produce(record['topic'], key=record['key'], value=record['value'], callback=DemoProducerCallback())\n",
    "\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fakerNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading Faker-35.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: confluent-kafka[avro] in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: avro<2,>=1.11.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from confluent-kafka[avro]) (1.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from confluent-kafka[avro]) (2.32.3)\n",
      "Requirement already satisfied: fastavro>=1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from confluent-kafka[avro]) (1.9.5)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from faker) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from faker) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\administrator\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->confluent-kafka[avro]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->confluent-kafka[avro]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->confluent-kafka[avro]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->confluent-kafka[avro]) (2024.7.4)\n",
      "Downloading Faker-35.2.2-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.6/1.9 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: faker\n",
      "Successfully installed faker-35.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install confluent-kafka[avro] faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'record',\n",
       " 'name': 'Customer',\n",
       " 'namespace': 'com.example',\n",
       " 'fields': [{'name': 'id', 'type': 'string'},\n",
       "  {'name': 'name', 'type': 'string'},\n",
       "  {'name': 'email', 'type': 'string'},\n",
       "  {'name': 'age', 'type': 'int'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Define the Avro Schema\n",
    "\n",
    "{  \n",
    " \"type\": \"record\",\n",
    "  \"name\": \"Customer\",\n",
    "  \"namespace\": \"com.example\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"id\", \"type\": \"string\"},    \n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"email\", \"type\": \"string\"},\n",
    "    {\"name\": \"age\", \"type\": \"int\"}\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer_generator.py\n",
    "customer_generator_code = \"\"\"\n",
    "from faker import Faker\n",
    "\n",
    "class CustomerGenerator:\n",
    "    def __init__(self):\n",
    "        self.fake = Faker()\n",
    "\n",
    "    def generate_customer(self):\n",
    "        return {\n",
    "            'name': self.fake.name(),\n",
    "            'email': self.fake.email(),\n",
    "            'address': self.fake.address(),\n",
    "            'phone_number': self.fake.phone_number(),\n",
    "            'birthdate': self.fake.date_of_birth(minimum_age=18, maximum_age=90).isoformat()\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "with open('customer_generator.py', 'w') as f:\n",
    "    f.write(customer_generator_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files 'customer_generator.py' and 'kafka_producer.py' have been created.\n"
     ]
    }
   ],
   "source": [
    "# Create kafka_producer.py\n",
    "kafka_producer_code = \"\"\"\n",
    "from confluent_kafka import SerializingProducer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from customer_generator import CustomerGenerator\n",
    "\n",
    "# Define the Avro schema for customer data\n",
    "customer_schema_str = '''\n",
    "{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Customer\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"},\n",
    "        {\"name\": \"address\", \"type\": \"string\"},\n",
    "        {\"name\": \"phone_number\", \"type\": \"string\"},\n",
    "        {\"name\": \"birthdate\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Configuration for Schema Registry\n",
    "schema_registry_conf = {\n",
    "    'url': 'http://10.222.68.223:8081'  # Replace with your Schema Registry URL\n",
    "}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# Avro serializer for customer data\n",
    "avro_serializer = AvroSerializer(\n",
    "    schema_registry_client,\n",
    "    customer_schema_str,\n",
    "    to_dict=lambda obj, ctx: obj  # Assuming the customer data is already in dictionary format\n",
    ")\n",
    "\n",
    "# Configuration for Kafka producer\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Replace with your Kafka broker URL\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer': avro_serializer\n",
    "}\n",
    "\n",
    "producer = SerializingProducer(producer_conf)\n",
    "\n",
    "# Function to produce customer data to Kafka\n",
    "def produce_customer_data():\n",
    "    customer_generator = CustomerGenerator()\n",
    "    customer_data = customer_generator.generate_customer()\n",
    "    producer.produce(topic='customer_topic', key=customer_data['email'], value=customer_data)\n",
    "    producer.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    produce_customer_data()\n",
    "\"\"\"\n",
    "\n",
    "with open('kafka_producer.py', 'w') as f:\n",
    "    f.write(kafka_producer_code)\n",
    "\n",
    "print(\"Files 'customer_generator.py' and 'kafka_producer.py' have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from confluent_kafka import SerializingProducer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastavroNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading fastavro-1.9.5-cp38-cp38-win_amd64.whl (550 kB)\n",
      "Installing collected packages: fastavro\n",
      "Successfully installed fastavro-1.9.5\n"
     ]
    }
   ],
   "source": [
    "pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaRegistryClient\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AvroSerializer\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\schema_registry\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (RegisteredSchema,\n\u001b[0;32m     19\u001b[0m                                      Schema,\n\u001b[0;32m     20\u001b[0m                                      SchemaRegistryClient,\n\u001b[0;32m     21\u001b[0m                                      SchemaRegistryError,\n\u001b[0;32m     22\u001b[0m                                      SchemaReference)\n\u001b[0;32m     24\u001b[0m _MAGIC_BYTE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegisteredSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchemaRegistryClient\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_record_subject_name_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecord_subject_name_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\schema_registry\\schema_registry_client.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lock\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Session,\n\u001b[0;32m     25\u001b[0m                       utils)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaRegistryError\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# TODO: consider adding `six` dependency or employing a compat file\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Python 2.7 is officially EOL so compatibility issue will be come more the norm.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# We need a better way to handle these issues.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# six: https://pypi.org/project/six/\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# compat file : https://github.com/psf/requests/blob/master/requests/compat.py\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent_kafka in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Producer, KafkaError\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringSerializer   \u001b[38;5;66;03m#, AvroSerializer\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaRegistryClient\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AvroSerializer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\schema_registry\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (RegisteredSchema,\n\u001b[0;32m     19\u001b[0m                                      Schema,\n\u001b[0;32m     20\u001b[0m                                      SchemaRegistryClient,\n\u001b[0;32m     21\u001b[0m                                      SchemaRegistryError,\n\u001b[0;32m     22\u001b[0m                                      SchemaReference)\n\u001b[0;32m     24\u001b[0m _MAGIC_BYTE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegisteredSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchemaRegistryClient\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_record_subject_name_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecord_subject_name_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\schema_registry\\schema_registry_client.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lock\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Session,\n\u001b[0;32m     25\u001b[0m                       utils)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaRegistryError\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# TODO: consider adding `six` dependency or employing a compat file\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Python 2.7 is officially EOL so compatibility issue will be come more the norm.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# We need a better way to handle these issues.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# six: https://pypi.org/project/six/\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# compat file : https://github.com/psf/requests/blob/master/requests/compat.py\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer, KafkaError\n",
    "from confluent_kafka.serialization import StringSerializer   #, AvroSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringSerializer   \u001b[38;5;66;03m#, AvroSerializer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#from confluent_kafka.schema_registry import SchemaRegistryClient\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AvroSerializer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\schema_registry\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (RegisteredSchema,\n\u001b[0;32m     19\u001b[0m                                      Schema,\n\u001b[0;32m     20\u001b[0m                                      SchemaRegistryClient,\n\u001b[0;32m     21\u001b[0m                                      SchemaRegistryError,\n\u001b[0;32m     22\u001b[0m                                      SchemaReference)\n\u001b[0;32m     24\u001b[0m _MAGIC_BYTE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegisteredSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSchemaRegistryClient\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic_record_subject_name_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecord_subject_name_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\schema_registry\\schema_registry_client.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lock\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Session,\n\u001b[0;32m     25\u001b[0m                       utils)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaRegistryError\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# TODO: consider adding `six` dependency or employing a compat file\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Python 2.7 is officially EOL so compatibility issue will be come more the norm.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# We need a better way to handle these issues.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# six: https://pypi.org/project/six/\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# compat file : https://github.com/psf/requests/blob/master/requests/compat.py\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer, KafkaError\n",
    "from confluent_kafka.serialization import StringSerializer   #, AvroSerializer\n",
    "#from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-win_amd64.whl (99 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.7.4 charset-normalizer-3.3.2 idna-3.7 requests-2.32.3 urllib3-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'confluent_kafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Producer, KafkaError\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringSerializer   \u001b[38;5;66;03m#, AvroSerializer\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_registry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SchemaRegistryClient\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'confluent_kafka'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer, KafkaError\n",
    "from confluent_kafka.serialization import StringSerializer   #, AvroSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the customer class (you can adjust this based on your actual schema)\n",
    "class Customer:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generator function to create sample Customer objects\n",
    "def generate_customers():\n",
    "    while True:\n",
    "        yield Customer(\"John\", 30)\n",
    "        yield Customer(\"Alice\", 25)\n",
    "        yield Customer(\"Bob\", 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'schema_registry_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Kafka producer configuration\u001b[39;00m\n\u001b[0;32m      2\u001b[0m producer_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap.servers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey.serializer\u001b[39m\u001b[38;5;124m'\u001b[39m: StringSerializer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf_8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue.serializer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mAvroSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_registry_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:8081\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m }\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'schema_registry_url'"
     ]
    }
   ],
   "source": [
    "# Initialize Kafka producer configuration\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer': AvroSerializer(schema_registry_url='http://localhost:8081')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'avro'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m avro\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AvroProducer, AvroConsumer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SerializerError\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\avro\\__init__.py:31\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcached_schema_registry_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CachedSchemaRegistryClient\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (SerializerError,  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     29\u001b[0m                                              KeySerializerError,\n\u001b[0;32m     30\u001b[0m                                              ValueSerializerError)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserializer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage_serializer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessageSerializer\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAvroProducer\u001b[39;00m(Producer):\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m        .. deprecated:: 2.0.2\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m        :param str default_value_schema: Optional default avro schema for value\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\avro\\serializer\\message_serializer.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mavro\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mavro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mavro\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientError\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'avro'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer, AvroConsumer         \n",
    "from confluent_kafka.avro.serializer import SerializerError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting avro\n",
      "  Downloading avro-1.11.3.tar.gz (90 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Building wheels for collected packages: avro\n",
      "  Building wheel for avro (PEP 517): started\n",
      "  Building wheel for avro (PEP 517): finished with status 'done'\n",
      "  Created wheel for avro: filename=avro-1.11.3-py2.py3-none-any.whl size=123927 sha256=3d18a96dc0a73e9e6b764c43c54a51c84560f356098d4844d99a94434491a837\n",
      "  Stored in directory: c:\\users\\administrator\\appdata\\local\\pip\\cache\\wheels\\13\\e8\\55\\db5c2684633ce754d4fae6d094ca60f13928c004f44c34c795\n",
      "Successfully built avro\n",
      "Installing collected packages: avro\n",
      "Successfully installed avro-1.11.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer, AvroConsumer\n",
    "from confluent_kafka.avro.serializer import SerializerError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_servers = 'localhost:9092'\n",
    "schema_registry_url = 'http://localhost:8081/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"MyRecord\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"field1\", \"type\": \"string\"}\n",
    "            # Add other fields as needed\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_9456\\1418123652.py:2: DeprecationWarning: CachedSchemaRegistry constructor is being deprecated. Use CachedSchemaRegistryClient(dict: config) instead. Existing params ca_location, cert_location and key_location will be replaced with their librdkafka equivalents as keys in the conf dict: `ssl.ca.location`, `ssl.certificate.location` and `ssl.key.location` respectively\n",
      "  schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n"
     ]
    }
   ],
   "source": [
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "value_schema = avro.loads(value_schema_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_serializer = MessageSerializer(schema_registry_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "encode_record_with_schema() missing 1 required positional argument: 'record'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m user_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfavorite_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfavorite_color\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m----> 2\u001b[0m serialized_data \u001b[38;5;241m=\u001b[39m \u001b[43mavro_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_record_with_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_schema_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: encode_record_with_schema() missing 1 required positional argument: 'record'"
     ]
    }
   ],
   "source": [
    "user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}\n",
    "serialized_data = avro_serializer.encode_record_with_schema(value_schema_str, user_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
