{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sample 1\n",
    "### #Basic Example of KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct Way to Use KafkaProducer in kafka-python   \n",
    "\n",
    "#pip install kafka \n",
    "#pip install kafka-python    \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='10.222.68.223:9092')\n",
    "producer.send('Test10', b'Hello, Kafka!')\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "kafka_props = {\n",
    "    'bootstrap_servers': '10.222.68.223:9092',\n",
    "    'key_serializer': str.encode,\n",
    "    'value_serializer': str.encode\n",
    "}\n",
    "\n",
    "producer = KafkaProducer(**kafka_props)\n",
    "\n",
    "try:\n",
    "    producer.send(\"CustomerCountry\", key=\"Precision Products\", value=\"France\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 3\n",
    "### Basic Example of KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Basic Example of KafkaConsumer\n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Create a consumer instance\n",
    "consumer = KafkaConsumer(\n",
    "    'Test10',\n",
    "    bootstrap_servers='10.222.68.223:9092',\n",
    "    auto_offset_reset='earliest', # or 'latest'\n",
    "    enable_auto_commit=True,\n",
    "    group_id='my-group',\n",
    "    value_deserializer=lambda x: x.decode('utf-8')\n",
    ")\n",
    "\n",
    "# Consume messages\n",
    "for message in consumer:\n",
    "    print(f\"Topic: {message.topic}, Partition: {message.partition}, Offset: {message.offset}\")\n",
    "    print(f\"Key: {message.key}, Value: {message.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 4\n",
    "### Using Confluent Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install confluent-kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "conf = {\n",
    "    'bootstrap.servers': '10.222.68.223:9092',\n",
    "    'client.id': 'my_producer'\n",
    "}\n",
    "producer = Producer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.produce(topic='my_topic', key='my_key', value='my_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 5\n",
    "### Using Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced message to CustomerCountry [0] @ offset 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "class DemoProducerCallback:\n",
    "    def __call__(self, err, msg):\n",
    "        if err is not None:\n",
    "            print(f\"Error: {err}\")\n",
    "        else:\n",
    "            print(f\"Produced message to {msg.topic()} [{msg.partition()}] @ offset {msg.offset()}\")\n",
    "\n",
    "producer = Producer({'bootstrap.servers': '10.222.68.223:9092'})\n",
    "\n",
    "record = {'topic': 'CustomerCountry', 'key': 'Biomedical Materials', 'value': 'BRAZIL'}\n",
    "producer.produce(record['topic'], key=record['key'], value=record['value'], callback=DemoProducerCallback())\n",
    "\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install confluent-kafka[avro] faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'record',\n",
       " 'name': 'Customer',\n",
       " 'namespace': 'com.example',\n",
       " 'fields': [{'name': 'id', 'type': 'string'},\n",
       "  {'name': 'name', 'type': 'string'},\n",
       "  {'name': 'email', 'type': 'string'},\n",
       "  {'name': 'age', 'type': 'int'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Define the Avro Schema\n",
    "\n",
    "{  \n",
    " \"type\": \"record\",\n",
    "  \"name\": \"Customer\",\n",
    "  \"namespace\": \"com.example\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"id\", \"type\": \"string\"},    \n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"email\", \"type\": \"string\"},\n",
    "    {\"name\": \"age\", \"type\": \"int\"}\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer_generator.py\n",
    "customer_generator_code = \"\"\"\n",
    "from faker import Faker\n",
    "\n",
    "class CustomerGenerator:\n",
    "    def __init__(self):\n",
    "        self.fake = Faker()\n",
    "\n",
    "    def generate_customer(self):\n",
    "        return {\n",
    "            'name': self.fake.name(),\n",
    "            'email': self.fake.email(),\n",
    "            'address': self.fake.address(),\n",
    "            'phone_number': self.fake.phone_number(),\n",
    "            'birthdate': self.fake.date_of_birth(minimum_age=18, maximum_age=90).isoformat()\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "with open('customer_generator.py', 'w') as f:\n",
    "    f.write(customer_generator_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files 'customer_generator.py' and 'kafka_producer.py' have been created.\n"
     ]
    }
   ],
   "source": [
    "# Create kafka_producer.py\n",
    "kafka_producer_code = \"\"\"\n",
    "from confluent_kafka import SerializingProducer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from customer_generator import CustomerGenerator\n",
    "\n",
    "# Define the Avro schema for customer data\n",
    "customer_schema_str = '''\n",
    "{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Customer\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"email\", \"type\": \"string\"},\n",
    "        {\"name\": \"address\", \"type\": \"string\"},\n",
    "        {\"name\": \"phone_number\", \"type\": \"string\"},\n",
    "        {\"name\": \"birthdate\", \"type\": \"string\"}\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Configuration for Schema Registry\n",
    "schema_registry_conf = {\n",
    "    'url': 'http://10.222.68.223:8081'  # Replace with your Schema Registry URL\n",
    "}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# Avro serializer for customer data\n",
    "avro_serializer = AvroSerializer(\n",
    "    schema_registry_client,\n",
    "    customer_schema_str,\n",
    "    to_dict=lambda obj, ctx: obj  # Assuming the customer data is already in dictionary format\n",
    ")\n",
    "\n",
    "# Configuration for Kafka producer\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',  # Replace with your Kafka broker URL\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer': avro_serializer\n",
    "}\n",
    "\n",
    "producer = SerializingProducer(producer_conf)\n",
    "\n",
    "# Function to produce customer data to Kafka\n",
    "def produce_customer_data():\n",
    "    customer_generator = CustomerGenerator()\n",
    "    customer_data = customer_generator.generate_customer()\n",
    "    producer.produce(topic='customer_topic', key=customer_data['email'], value=customer_data)\n",
    "    producer.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    produce_customer_data()\n",
    "\"\"\"\n",
    "\n",
    "with open('kafka_producer.py', 'w') as f:\n",
    "    f.write(kafka_producer_code)\n",
    "\n",
    "print(\"Files 'customer_generator.py' and 'kafka_producer.py' have been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastavroNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading fastavro-1.9.5-cp38-cp38-win_amd64.whl (550 kB)\n",
      "Installing collected packages: fastavro\n",
      "Successfully installed fastavro-1.9.5\n"
     ]
    }
   ],
   "source": [
    "pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "from confluent_kafka import SerializingProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Schema Registry configuration\n",
    "schema_registry_conf = {'url': 'http://10.222.68.223:8081'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your Avro schema (as a string or load from file)\n",
    "avro_schema_str = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Customer\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"id\", \"type\": \"string\"},\n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"email\", \"type\": \"string\"}\n",
    "     ]\n",
    "} \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Avro serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, avro_schema_str)\n",
    "\n",
    "# Kafka producer configuration\n",
    "producer_config = {\n",
    "    'bootstrap.servers': '10.222.68.223:9092',\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer': avro_serializer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "producer = SerializingProducer(producer_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import uuid\n",
    "\n",
    "# Define the value to send (must match the Avro schema)\n",
    "customer_data = {\n",
    "    \"id\": str(uuid.uuid4()),\n",
    "    \"name\": \"Alice Johnson\",\n",
    "    \"email\": \"alice.johnson@example.com\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery successful: 099eedf5-ff04-4af3-9977-524eb350346d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Send the message\n",
    "producer.produce(\n",
    "    topic=\"customers\",\n",
    "    key=customer_data[\"id\"],\n",
    "    value=customer_data,\n",
    "    on_delivery=lambda err, msg: print(\n",
    "         f\"Delivery {'failed: ' + str(err) if err else 'successful: ' + msg.key().decode('utf-8')}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wait for all messages to be delivered\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: avro in c:\\users\\administrator\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.11.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_servers = '10.222.68.223:9092'\n",
    "schema_registry_url = 'http://10.222.68.223:8081/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"MyRecord\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"field1\", \"type\": \"string\"}\n",
    "            # Add other fields as needed\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_config = {\n",
    "    'bootstrap.servers': bootstrap_servers,\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer, AvroDeserializer\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_registry_conf = {'url': 'http://10.222.68.223:8081'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define schema and data\n",
    "\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Avro serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, value_schema_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}\n",
    "#serialized_data = avro_serializer.encode_record_with_schema(value_schema_str, user_data)\n",
    "\n",
    "\n",
    "# Serialize the data\n",
    "serialized_data = avro_serializer(user_data, SerializationContext(\"MyTopic3\", MessageField.VALUE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_deserializer = AvroDeserializer(schema_registry_client)\n",
    "deserialized_data = avro_deserializer(serialized_data, SerializationContext(\"MyTopic3\", MessageField.VALUE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```bash\n",
    "#pip install confluent-kafka[avro]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### 📦 Step 1: Define the Avro Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': 'example.avro',\n",
       " 'type': 'record',\n",
       " 'name': 'User',\n",
       " 'fields': [{'name': 'name', 'type': 'string'},\n",
       "  {'name': 'age', 'type': 'int'},\n",
       "  {'name': 'email', 'type': 'string'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#user.avsc \n",
    "{\n",
    "  \"namespace\": \"example.avro\",\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"User\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"age\", \"type\": \"int\"},\n",
    "    {\"name\": \"email\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 📝 Step 2: Producer Code (Serialize and Send)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_11124\\187638522.py:12: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  producer = AvroProducer(producer_config, default_value_schema=value_schema)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message produced successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka import avro\n",
    "\n",
    "value_schema = avro.load('user.avsc')\n",
    "\n",
    "producer_config = {\n",
    "    'bootstrap.servers': '10.222.68.223:9092',\n",
    "    'schema.registry.url': 'http://10.222.68.223:8081'\n",
    "}\n",
    "\n",
    "producer = AvroProducer(producer_config, default_value_schema=value_schema)\n",
    "\n",
    "value = {\"name\": \"Alice\", \"age\": 30, \"email\": \"alice@example.com\"}\n",
    "\n",
    "try:\n",
    "    producer.produce(topic='users', value=value)\n",
    "    producer.flush()\n",
    "    print(\"Message produced successfully.\")\n",
    "except SerializerError as e:\n",
    "    print(f\"Message serialization failed: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 📥 Step 3: Consumer Code (Deserialize and Read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_11124\\1976440200.py:10: DeprecationWarning: AvroConsumer has been deprecated. Use AvroDeserializer instead.\n",
      "  consumer = AvroConsumer(consumer_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for messages...\n",
      "Received message: {'name': 'Alice', 'age': 30, 'email': 'alice@example.com'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from confluent_kafka.avro import AvroConsumer\n",
    "\n",
    "consumer_config = {\n",
    "    'bootstrap.servers': '10.222.68.223:9092',\n",
    "    'group.id': 'user-consumer-group',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'schema.registry.url': 'http://10.222.68.223:8081'\n",
    "}\n",
    "\n",
    "consumer = AvroConsumer(consumer_config)\n",
    "consumer.subscribe(['users'])\n",
    "\n",
    "print(\"Waiting for messages...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Received message: {msg.value()}\")\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
