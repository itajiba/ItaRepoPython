{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_8640\\2654678579.py:14: DeprecationWarning: CachedSchemaRegistry constructor is being deprecated. Use CachedSchemaRegistryClient(dict: config) instead. Existing params ca_location, cert_location and key_location will be replaced with their librdkafka equivalents as keys in the conf dict: `ssl.ca.location`, `ssl.certificate.location` and `ssl.key.location` respectively\n",
      "  schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_8640\\2654678579.py:39: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  producer = AvroProducer(producer_config, default_value_schema=value_schema)\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "\n",
    "#from confluent_kafka.avro.serializer import SerializerError, MessageSerializer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up Schema Registry client\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
    "\n",
    "# Define Avro schema\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "value_schema = avro.loads(value_schema_str)\n",
    "\n",
    "# Create Avro serializer\n",
    "avro_serializer = MessageSerializer(schema_registry_client)\n",
    "\n",
    "# Create Avro Producer\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}\n",
    "producer = AvroProducer(producer_config, default_value_schema=value_schema)\n",
    "\n",
    "# Create user data\n",
    "user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the message\n",
    "producer.produce(topic='my_avro_topic', value=user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_8640\\2559264547.py:15: DeprecationWarning: CachedSchemaRegistry constructor is being deprecated. Use CachedSchemaRegistryClient(dict: config) instead. Existing params ca_location, cert_location and key_location will be replaced with their librdkafka equivalents as keys in the conf dict: `ssl.ca.location`, `ssl.certificate.location` and `ssl.key.location` respectively\n",
      "  schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_8640\\2559264547.py:40: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  producer = AvroProducer(producer_config, default_value_schema=value_schema)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message produced successfully with Avro schema.\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "\n",
    "\n",
    "#from confluent_kafka.avro.serializer import SerializerError, MessageSerializer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up Schema Registry client\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
    "\n",
    "# Define Avro schema\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "value_schema = avro.loads(value_schema_str)\n",
    "\n",
    "# Create Avro serializer\n",
    "avro_serializer = MessageSerializer(schema_registry_client)\n",
    "\n",
    "# Create Avro Producer\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}\n",
    "producer = AvroProducer(producer_config, default_value_schema=value_schema)\n",
    "\n",
    "# Create user data\n",
    "user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}\n",
    "\n",
    "# Produce the message\n",
    "producer.produce(topic='my_avro_topic', value=user_data)\n",
    "\n",
    "# Flush the producer to ensure the message is sent\n",
    "producer.flush()\n",
    "\n",
    "print(\"Message produced successfully with Avro schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'schema_registry_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m Customer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize Kafka producer configuration\u001b[39;00m\n\u001b[0;32m     21\u001b[0m producer_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap.servers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey.serializer\u001b[39m\u001b[38;5;124m'\u001b[39m: StringSerializer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf_8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue.serializer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mAvroSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_registry_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:8081\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m }\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Create a Kafka producer\u001b[39;00m\n\u001b[0;32m     28\u001b[0m producer \u001b[38;5;241m=\u001b[39m Producer(producer_config)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'schema_registry_url'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer, KafkaError\n",
    "from confluent_kafka.serialization import StringSerializer   #, AvroSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import json\n",
    "\n",
    "# Define the customer class (you can adjust this based on your actual schema)\n",
    "class Customer:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "# Define a generator function to create sample Customer objects\n",
    "def generate_customers():\n",
    "    while True:\n",
    "        yield Customer(\"John\", 30)\n",
    "        yield Customer(\"Alice\", 25)\n",
    "        yield Customer(\"Bob\", 28)\n",
    "\n",
    "# Initialize Kafka producer configuration\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer': AvroSerializer(schema_registry_url='http://localhost:8081')\n",
    "}\n",
    "\n",
    "# Create a Kafka producer\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# Define the topic\n",
    "topic = \"customerContacts\"\n",
    "\n",
    "# Create a generator for sample customers\n",
    "customer_generator = generate_customers()\n",
    "\n",
    "try:\n",
    "    # Produce new events until manually stopped\n",
    "    while True:\n",
    "        customer = next(customer_generator)\n",
    "        print(f\"Generated customer: {customer.name}, {customer.age}\")\n",
    "        producer.produce(topic, key=customer.name, value=customer)\n",
    "        producer.flush()  # Ensure the message is sent immediately\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Producer stopped manually.\")\n",
    "\n",
    "# Close the producer\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KafkaAvroSerializer' from 'confluent_kafka.serialization' (c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\serialization\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaError\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#import io.confluent.kafka.serializers\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaAvroSerializer, KafkaAvroDeserializer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustomer_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomerGenerator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a Kafka producer with specified properties\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KafkaAvroSerializer' from 'confluent_kafka.serialization' (c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\serialization\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "#import io.confluent.kafka.serializers\n",
    "from confluent_kafka.serialization import KafkaAvroSerializer, KafkaAvroDeserializer\n",
    "from customer_generator import CustomerGenerator\n",
    "\n",
    "# Create a Kafka producer with specified properties\n",
    "props = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'key.serializer': KafkaAvroSerializer,\n",
    "    'value.serializer': KafkaAvroSerializer,\n",
    "    'schema.registry.url': schemaUrl #'http://localhost:8081'\n",
    "}\n",
    "producer = KafkaProducer(**props)\n",
    "consumer = Consumer(**props)\n",
    "\n",
    "# Define the topic\n",
    "topic = 'customerContacts'\n",
    "\n",
    "# Keep producing new events until someone interrupts\n",
    "while True:\n",
    "    customer = CustomerGenerator.getNext()\n",
    "    print(f\"Generated customer {customer}\")\n",
    "    record = producer.send(topic, key=customer.getName(), value=customer)\n",
    "    try:\n",
    "        record.get(timeout=10)\n",
    "    except KafkaError:\n",
    "        print(\"Error sending record to Kafka\")\n",
    "\t\t\n",
    "\n",
    "\n",
    "msg = consumer.poll(1.0)\n",
    "if msg:\n",
    "    key = msg.key()\n",
    "    value = msg.value()\n",
    "    print(f\"Received message: key={key}, value={value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "You must pass either schema string or schema object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m      9\u001b[0m avro_schema \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecord\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     ]\n\u001b[0;32m     16\u001b[0m }\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create an Avro serializer\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m avro_serializer \u001b[38;5;241m=\u001b[39m \u001b[43mAvroSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_registry_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavro_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Your data (example: a dictionary)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m user_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m30\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\schema_registry\\avro.py:193\u001b[0m, in \u001b[0;36mAvroSerializer.__init__\u001b[1;34m(self, schema_registry_client, schema_str, to_dict, conf)\u001b[0m\n\u001b[0;32m    191\u001b[0m     schema \u001b[38;5;241m=\u001b[39m schema_str\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must pass either schema string or schema object\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry \u001b[38;5;241m=\u001b[39m schema_registry_client\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: You must pass either schema string or schema object"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "\n",
    "# Initialize the schema registry client\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = SchemaRegistryClient({'url': schema_registry_url})\n",
    "\n",
    "# Define your Avro schema (replace with your actual schema)\n",
    "avro_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create an Avro serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, avro_schema)\n",
    "\n",
    "# Your data (example: a dictionary)\n",
    "user_data = {\"name\": \"Alice\", \"age\": 30}\n",
    "\n",
    "# Serialize the data\n",
    "serialized_data = avro_serializer.serialize(topic='my_topic', data=user_data)\n",
    "\n",
    "# Now you can send the serialized data to Kafka\n",
    "# (e.g., using the KafkaProducer from confluent-kafka-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AvroSerializer' object has no attribute 'serialize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m user_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m30\u001b[39m}\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Serialize the data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m serialized_data \u001b[38;5;241m=\u001b[39m \u001b[43mavro_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m(topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_topic\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39muser_data)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Now you can send the serialized data to Kafka\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# (e.g., using the KafkaProducer from confluent-kafka-python)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AvroSerializer' object has no attribute 'serialize'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "\n",
    "# Initialize the schema registry client\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = SchemaRegistryClient({'url': schema_registry_url})\n",
    "\n",
    "# Define your Avro schema (replace with your actual schema)\n",
    "avro_schema = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "# Create an Avro serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, avro_schema)\n",
    "\n",
    "# Your data (example: a dictionary)\n",
    "user_data = {\"name\": \"Alice\", \"age\": 30}\n",
    "\n",
    "# Serialize the data\n",
    "serialized_data = avro_serializer.serialize(topic='my_topic', data=user_data)\n",
    "\n",
    "# Now you can send the serialized data to Kafka\n",
    "# (e.g., using the KafkaProducer from confluent-kafka-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m30\u001b[39m}\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Serialize the data\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m serialized_data \u001b[38;5;241m=\u001b[39m \u001b[43mfastavro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschemaless_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Deserialize the data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m deserialized_data \u001b[38;5;241m=\u001b[39m fastavro\u001b[38;5;241m.\u001b[39mschemaless_reader(serialized_data, schema)\n",
      "File \u001b[1;32mfastavro\\\\_write.pyx:817\u001b[0m, in \u001b[0;36mfastavro._write.schemaless_writer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "import fastavro\n",
    "\n",
    "# Define your Avro schema (e.g., as a dictionary)\n",
    "schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Person\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create some data\n",
    "data = {\"name\": \"Alice\", \"age\": 30}\n",
    "\n",
    "# Serialize the data\n",
    "serialized_data = fastavro.schemaless_writer(None, schema, data)\n",
    "\n",
    "# Deserialize the data\n",
    "deserialized_data = fastavro.schemaless_reader(serialized_data, schema)\n",
    "\n",
    "print(deserialized_data)  # Output: {'name': 'Alice', 'age': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "kafka_props = {\n",
    "    'bootstrap_servers': 'localhost:9092',\n",
    "    'key_serializer': str.encode,\n",
    "    'value_serializer': str.encode\n",
    "}\n",
    "\n",
    "producer = KafkaProducer(**kafka_props)\n",
    "\n",
    "#record = ProducerRecord(\"CustomerCountry\", \"Precision Products\", \"France\")\n",
    "\n",
    "try:\n",
    "    producer.send(\"CustomerCountry\", key=\"Precision Products\", value=\"France\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KafkaAvroSerializer' from 'confluent_kafka.serialization' (c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\serialization\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaError\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#import io.confluent.kafka.serializers\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaAvroSerializer, KafkaAvroDeserializer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustomer_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomerGenerator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a Kafka producer with specified properties\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KafkaAvroSerializer' from 'confluent_kafka.serialization' (c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\confluent_kafka\\serialization\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "#import io.confluent.kafka.serializers\n",
    "from confluent_kafka.serialization import KafkaAvroSerializer, KafkaAvroDeserializer\n",
    "from customer_generator import CustomerGenerator\n",
    "\n",
    "# Create a Kafka producer with specified properties\n",
    "props = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'key.serializer': KafkaAvroSerializer,\n",
    "    'value.serializer': KafkaAvroSerializer,\n",
    "    'schema.registry.url': schemaUrl #'http://localhost:8081'\n",
    "}\n",
    "producer = KafkaProducer(**props)\n",
    "consumer = Consumer(**props)\n",
    "\n",
    "# Define the topic\n",
    "topic = 'customerContacts'\n",
    "\n",
    "# Keep producing new events until someone interrupts\n",
    "while True:\n",
    "    customer = CustomerGenerator.getNext()\n",
    "    print(f\"Generated customer {customer}\")\n",
    "    record = producer.send(topic, key=customer.getName(), value=customer)\n",
    "    try:\n",
    "        record.get(timeout=10)\n",
    "    except KafkaError:\n",
    "        print(\"Error sending record to Kafka\")\n",
    "\t\t\n",
    "\n",
    "\n",
    "msg = consumer.poll(1.0)\n",
    "if msg:\n",
    "    key = msg.key()\n",
    "    value = msg.value()\n",
    "    print(f\"Received message: key={key}, value={value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'schema_registry_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m Customer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize Kafka producer configuration\u001b[39;00m\n\u001b[0;32m     21\u001b[0m producer_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap.servers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost:9092\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey.serializer\u001b[39m\u001b[38;5;124m'\u001b[39m: StringSerializer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf_8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue.serializer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mAvroSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_registry_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:8081\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m }\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Create a Kafka producer\u001b[39;00m\n\u001b[0;32m     28\u001b[0m producer \u001b[38;5;241m=\u001b[39m Producer(producer_config)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'schema_registry_url'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer, KafkaError\n",
    "from confluent_kafka.serialization import StringSerializer   #, AvroSerializer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import json\n",
    "\n",
    "# Define the customer class (you can adjust this based on your actual schema)\n",
    "class Customer:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "# Define a generator function to create sample Customer objects\n",
    "def generate_customers():\n",
    "    while True:\n",
    "        yield Customer(\"John\", 30)\n",
    "        yield Customer(\"Alice\", 25)\n",
    "        yield Customer(\"Bob\", 28)\n",
    "\n",
    "# Initialize Kafka producer configuration\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer': AvroSerializer(schema_registry_url='http://localhost:8081')\n",
    "}\n",
    "\n",
    "# Create a Kafka producer\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# Define the topic\n",
    "topic = \"customerContacts\"\n",
    "\n",
    "# Create a generator for sample customers\n",
    "customer_generator = generate_customers()\n",
    "\n",
    "try:\n",
    "    # Produce new events until manually stopped\n",
    "    while True:\n",
    "        customer = next(customer_generator)\n",
    "        print(f\"Generated customer: {customer.name}, {customer.age}\")\n",
    "        producer.produce(topic, key=customer.name, value=customer)\n",
    "        producer.flush()  # Ensure the message is sent immediately\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Producer stopped manually.\")\n",
    "\n",
    "# Close the producer\n",
    "producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AvroSerializer' object has no attribute 'serialize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m user_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m30\u001b[39m}\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Serialize the data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m serialized_data \u001b[38;5;241m=\u001b[39m \u001b[43mavro_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m(topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_topic\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39muser_data)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Now you can send the serialized data to Kafka\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# (e.g., using the KafkaProducer from confluent-kafka-python)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AvroSerializer' object has no attribute 'serialize'"
     ]
    }
   ],
   "source": [
    " from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "\n",
    "# Initialize the schema registry client\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = SchemaRegistryClient({'url': schema_registry_url})\n",
    "\n",
    "# Define your Avro schema (replace with your actual schema)\n",
    "avro_schema = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"age\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "# Create an Avro serializer\n",
    "avro_serializer = AvroSerializer(schema_registry_client, avro_schema)\n",
    "\n",
    "# Your data (example: a dictionary)\n",
    "user_data = {\"name\": \"Alice\", \"age\": 30}\n",
    "\n",
    "# Serialize the data\n",
    "serialized_data = avro_serializer.serialize(topic='my_topic', data=user_data)\n",
    "\n",
    "# Now you can send the serialized data to Kafka\n",
    "# (e.g., using the KafkaProducer from confluent-kafka-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3433435798.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install confluent-kafka avro-python3\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install confluent-kafka avro-python3\n",
    "\n",
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
    "\n",
    "\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "value_schema = avro.loads(value_schema_str)\n",
    "\n",
    "avro_serializer = MessageSerializer(schema_registry_client)\n",
    "\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}\n",
    "producer = AvroProducer(producer_config, default_value_schema=value_schema)\n",
    "\n",
    "\n",
    "\n",
    "user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}\n",
    "serialized_data = avro_serializer.encode_record_with_schema(value_schema, user_data)\n",
    "\n",
    "# Produce the message\n",
    "producer.produce(topic='my_avro_topic', value=serialized_data)\n",
    "\n",
    "# Flush the producer to ensure the message is sent\n",
    "producer.flush()\n",
    "\n",
    "print(\"Message produced successfully with Avro schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6560\\1945269914.py:7: DeprecationWarning: CachedSchemaRegistry constructor is being deprecated. Use CachedSchemaRegistryClient(dict: config) instead. Existing params ca_location, cert_location and key_location will be replaced with their librdkafka equivalents as keys in the conf dict: `ssl.ca.location`, `ssl.certificate.location` and `ssl.key.location` respectively\n",
      "  schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6560\\1945269914.py:30: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  producer = AvroProducer(producer_config, default_value_schema=value_schema)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "encode_record_with_schema() missing 1 required positional argument: 'record'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 35\u001b[0m\n\u001b[0;32m     30\u001b[0m producer \u001b[38;5;241m=\u001b[39m AvroProducer(producer_config, default_value_schema\u001b[38;5;241m=\u001b[39mvalue_schema)\n\u001b[0;32m     34\u001b[0m user_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfavorite_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfavorite_color\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m---> 35\u001b[0m serialized_data \u001b[38;5;241m=\u001b[39m \u001b[43mavro_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_record_with_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Produce the message\u001b[39;00m\n\u001b[0;32m     38\u001b[0m producer\u001b[38;5;241m.\u001b[39mproduce(topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_avro_topic\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39mserialized_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: encode_record_with_schema() missing 1 required positional argument: 'record'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
    "\n",
    "\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "value_schema = avro.loads(value_schema_str)\n",
    "\n",
    "avro_serializer = MessageSerializer(schema_registry_client)\n",
    "\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}\n",
    "producer = AvroProducer(producer_config, default_value_schema=value_schema)\n",
    "\n",
    "\n",
    "\n",
    "user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}\n",
    "serialized_data = avro_serializer.encode_record_with_schema(value_schema, user_data)\n",
    "\n",
    "# Produce the message\n",
    "producer.produce(topic='my_avro_topic', value=serialized_data)\n",
    "\n",
    "# Flush the producer to ensure the message is sent\n",
    "producer.flush()\n",
    "\n",
    "print(\"Message produced successfully with Avro schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6560\\1917980186.py:8: DeprecationWarning: CachedSchemaRegistry constructor is being deprecated. Use CachedSchemaRegistryClient(dict: config) instead. Existing params ca_location, cert_location and key_location will be replaced with their librdkafka equivalents as keys in the conf dict: `ssl.ca.location`, `ssl.certificate.location` and `ssl.key.location` respectively\n",
      "  schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6560\\1917980186.py:33: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  producer = AvroProducer(producer_config, default_value_schema=value_schema)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "encode_record_with_schema() missing 1 required positional argument: 'record'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Create user data and serialize it\u001b[39;00m\n\u001b[0;32m     36\u001b[0m user_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfavorite_number\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfavorite_color\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m---> 37\u001b[0m serialized_data \u001b[38;5;241m=\u001b[39m \u001b[43mavro_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_record_with_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Produce the message\u001b[39;00m\n\u001b[0;32m     40\u001b[0m producer\u001b[38;5;241m.\u001b[39mproduce(topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_avro_topic\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39mserialized_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: encode_record_with_schema() missing 1 required positional argument: 'record'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "\n",
    "# Set up Schema Registry client\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
    "\n",
    "# Define Avro schema\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "value_schema = avro.loads(value_schema_str)\n",
    "\n",
    "# Create Avro serializer\n",
    "avro_serializer = MessageSerializer(schema_registry_client)\n",
    "\n",
    "# Create Avro Producer\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}\n",
    "producer = AvroProducer(producer_config, default_value_schema=value_schema)\n",
    "\n",
    "# Create user data and serialize it\n",
    "user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}\n",
    "serialized_data = avro_serializer.encode_record_with_schema(value_schema, user_data)\n",
    "\n",
    "# Produce the message\n",
    "producer.produce(topic='my_avro_topic', value=serialized_data)\n",
    "\n",
    "# Flush the producer to ensure the message is sent\n",
    "producer.flush()\n",
    "\n",
    "print(\"Message produced successfully with Avro schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6560\\2559264547.py:15: DeprecationWarning: CachedSchemaRegistry constructor is being deprecated. Use CachedSchemaRegistryClient(dict: config) instead. Existing params ca_location, cert_location and key_location will be replaced with their librdkafka equivalents as keys in the conf dict: `ssl.ca.location`, `ssl.certificate.location` and `ssl.key.location` respectively\n",
      "  schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6560\\2559264547.py:40: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  producer = AvroProducer(producer_config, default_value_schema=value_schema)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message produced successfully with Avro schema.\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "from confluent_kafka.avro.serializer import SerializerError\n",
    "from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\n",
    "\n",
    "\n",
    "#from confluent_kafka.avro.serializer import SerializerError, MessageSerializer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up Schema Registry client\n",
    "schema_registry_url = 'http://localhost:8081'\n",
    "schema_registry_client = avro.CachedSchemaRegistryClient(schema_registry_url)\n",
    "\n",
    "# Define Avro schema\n",
    "value_schema_str = \"\"\"\n",
    "{\n",
    "    \"namespace\": \"example.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"name\", \"type\": \"string\"},\n",
    "        {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]},\n",
    "        {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "value_schema = avro.loads(value_schema_str)\n",
    "\n",
    "# Create Avro serializer\n",
    "avro_serializer = MessageSerializer(schema_registry_client)\n",
    "\n",
    "# Create Avro Producer\n",
    "producer_config = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'schema.registry.url': schema_registry_url\n",
    "}\n",
    "producer = AvroProducer(producer_config, default_value_schema=value_schema)\n",
    "\n",
    "# Create user data\n",
    "user_data = {\"name\": \"Alice\", \"favorite_number\": 42, \"favorite_color\": \"blue\"}\n",
    "\n",
    "# Produce the message\n",
    "producer.produce(topic='my_avro_topic', value=user_data)\n",
    "\n",
    "# Flush the producer to ensure the message is sent\n",
    "producer.flush()\n",
    "\n",
    "print(\"Message produced successfully with Avro schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced message to CustomerCountry [0] @ offset 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "class DemoProducerCallback:\n",
    "    def __call__(self, err, msg):\n",
    "        if err is not None:\n",
    "            print(f\"Error: {err}\")\n",
    "        else:\n",
    "            print(f\"Produced message to {msg.topic()} [{msg.partition()}] @ offset {msg.offset()}\")\n",
    "\n",
    "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
    "\n",
    "record = {'topic': 'CustomerCountry', 'key': 'Biomedical Materials', 'value': 'USA'}\n",
    "producer.produce(record['topic'], key=record['key'], value=record['value'], callback=DemoProducerCallback())\n",
    "\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "kafka_props = {\n",
    "    'bootstrap_servers': 'localhost:9092',\n",
    "    'key_serializer': str.encode,\n",
    "    'value_serializer': str.encode\n",
    "}\n",
    "\n",
    "producer = KafkaProducer(**kafka_props)\n",
    "\n",
    "#record = ProducerRecord(\"CustomerCountry\", \"Precision Products\", \"France\")\n",
    "\n",
    "try:\n",
    "    producer.send(\"CustomerCountry\", key=\"Precision Products\", value=\"France\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'confluent_kafka.partitioner'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Producer, KafkaException\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfluent_kafka\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartitioner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Partitioner\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmmh3\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'confluent_kafka.partitioner'"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer, KafkaException\n",
    "from confluent_kafka.partitioner import Partitioner\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement confluent_kafka.partitioner (from versions: none)\n",
      "ERROR: No matching distribution found for confluent_kafka.partitioner\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka.partitioner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mmh3\n",
      "  Downloading mmh3-4.1.0-cp38-cp38-win_amd64.whl.metadata (13 kB)\n",
      "Downloading mmh3-4.1.0-cp38-cp38-win_amd64.whl (31 kB)\n",
      "Installing collected packages: mmh3\n",
      "Successfully installed mmh3-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mmh3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, KafkaException\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BananaPartitioner:\n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "\n",
    "    def __call__(self, key, partitions, msg):\n",
    "        if key is None or not isinstance(key, str):\n",
    "            raise KafkaException(\"We expect all messages to have customer name as key\")\n",
    "\n",
    "        num_partitions = len(partitions)\n",
    "        if key == \"Banana\":\n",
    "            return partitions[-1]  # Banana will always go to the last partition\n",
    "\n",
    "        # Other records will get hashed to the rest of the partitions\n",
    "        return partitions[abs(mmh3.hash(key)) % (num_partitions - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'partitioner': BananaPartitioner\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KafkaException",
     "evalue": "KafkaError{code=_INVALID_ARG,val=-186,str=\"Invalid value for configuration property \"partitioner\": <class '__main__.BananaPartitioner'>\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKafkaException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m producer \u001b[38;5;241m=\u001b[39m \u001b[43mProducer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKafkaException\u001b[0m: KafkaError{code=_INVALID_ARG,val=-186,str=\"Invalid value for configuration property \"partitioner\": <class '__main__.BananaPartitioner'>\"}"
     ]
    }
   ],
   "source": [
    "producer = Producer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092'\n",
    "}\n",
    "\n",
    "producer = Producer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_message(topic, key, value):\n",
    "    partitions = producer.list_topics().topics[topic].partitions.keys()\n",
    "    partition = banana_partitioner(key, list(partitions))\n",
    "    producer.produce(topic, key=key, value=value, partition=partition, callback=delivery_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'my_topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mproduce_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_topic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBanana\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBanana message\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m, in \u001b[0;36mproduce_message\u001b[1;34m(topic, key, value)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproduce_message\u001b[39m(topic, key, value):\n\u001b[1;32m----> 2\u001b[0m     partitions \u001b[38;5;241m=\u001b[39m \u001b[43mproducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpartitions\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m      3\u001b[0m     partition \u001b[38;5;241m=\u001b[39m banana_partitioner(key, \u001b[38;5;28mlist\u001b[39m(partitions))\n\u001b[0;32m      4\u001b[0m     producer\u001b[38;5;241m.\u001b[39mproduce(topic, key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39mvalue, partition\u001b[38;5;241m=\u001b[39mpartition, callback\u001b[38;5;241m=\u001b[39mdelivery_report)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'my_topic'"
     ]
    }
   ],
   "source": [
    "produce_message('my_topic', 'Banana', 'Banana message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'my_topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mproduce_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_topic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApple\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApple message\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m, in \u001b[0;36mproduce_message\u001b[1;34m(topic, key, value)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproduce_message\u001b[39m(topic, key, value):\n\u001b[1;32m----> 2\u001b[0m     partitions \u001b[38;5;241m=\u001b[39m \u001b[43mproducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpartitions\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m      3\u001b[0m     partition \u001b[38;5;241m=\u001b[39m banana_partitioner(key, \u001b[38;5;28mlist\u001b[39m(partitions))\n\u001b[0;32m      4\u001b[0m     producer\u001b[38;5;241m.\u001b[39mproduce(topic, key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39mvalue, partition\u001b[38;5;241m=\u001b[39mpartition, callback\u001b[38;5;241m=\u001b[39mdelivery_report)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'my_topic'"
     ]
    }
   ],
   "source": [
    "produce_message('my_topic', 'Apple', 'Apple message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'banana_partitioner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mproduce_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_topic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBanana\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBanana message\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m, in \u001b[0;36mproduce_message\u001b[1;34m(topic, key, value)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproduce_message\u001b[39m(topic, key, value):\n\u001b[0;32m      2\u001b[0m     partitions \u001b[38;5;241m=\u001b[39m producer\u001b[38;5;241m.\u001b[39mlist_topics()\u001b[38;5;241m.\u001b[39mtopics[topic]\u001b[38;5;241m.\u001b[39mpartitions\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m----> 3\u001b[0m     partition \u001b[38;5;241m=\u001b[39m \u001b[43mbanana_partitioner\u001b[49m(key, \u001b[38;5;28mlist\u001b[39m(partitions))\n\u001b[0;32m      4\u001b[0m     producer\u001b[38;5;241m.\u001b[39mproduce(topic, key\u001b[38;5;241m=\u001b[39mkey, value\u001b[38;5;241m=\u001b[39mvalue, partition\u001b[38;5;241m=\u001b[39mpartition, callback\u001b[38;5;241m=\u001b[39mdelivery_report)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'banana_partitioner' is not defined"
     ]
    }
   ],
   "source": [
    "produce_message('my_topic', 'Banana', 'Banana message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
